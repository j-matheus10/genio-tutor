# app.py
import os
import streamlit as st
from google import genai
from google.genai import types
# FIX 1: Importar Chroma de la ruta 'community' (soluciona ModuleNotFoundError)
from langchain_community.vectorstores import Chroma
# FIX 2: Importar la clase de Embeddings con el nombre completo (soluciona ImportError/AttributeError)
from langchain_google_genai import GoogleGenerativeAIEmbeddings 

# --- CONFIGURACI√ìN DE VARIABLES GLOBALES ---
MODELO = "gemini-2.5-flash" 
# Ruta donde Streamlit esperar√° encontrar la base de datos RAG (carpeta genio_db_knowledge)
DB_PATH = "genio_db_knowledge" 
EMBEDDING_MODEL_NAME = "text-embedding-004" 

# --- PERSONALIDAD DEL TUTOR (MODO DUAL) ---
SYSTEM_INSTRUCTION = """
Eres "Genio", un tutor socr√°tico y asistente resolutivo. Tu objetivo es alternar entre dos modos, seg√∫n lo solicite el usuario.

--- REGLAS DE MODO ---
1. MODO ENSE√ëAR (Predeterminado):
   - Objetivo: Fomentar el aprendizaje y la autonom√≠a.
   - Regla de Oro: NUNCA dar la respuesta directa. Usar preguntas gu√≠a y el m√©todo socr√°tico. Ser motivador (emojis).

2. MODO RESOLVER (Gu√≠a Resolutivo):
   - Objetivo: Proveer la soluci√≥n clara o un resumen de hechos.
   - Activaci√≥n: Si el usuario dice 'Modo: Resolver', 'Dame la respuesta', o 'Resu√©lvelo'.
   - Regla de Oro: Ofrecer la respuesta directa, clara y paso a paso.

--- REGLAS GLOBALES ---
- Usa **negritas** para destacar las palabras clave.
- Utiliza el CONTEXTO RAG provisto para responder o guiar.
"""

# Configuraci√≥n base del chat
chat_config = types.GenerateContentConfig(
    system_instruction=SYSTEM_INSTRUCTION,
    temperature=0.7,
    max_output_tokens=1000
)

# --- INICIALIZACI√ìN DE RECURSOS (CACH√â) ---

@st.cache_resource
def initialize_gemini():
    """Inicializa el cliente de Gemini y la funci√≥n de embeddings."""
    # 1. Cargar API Key desde Streamlit Secrets
    try:
        api_key = st.secrets["GEMINI_API_KEY"]
    except KeyError:
        # Se detiene si la clave no est√° en los secretos de Streamlit
        st.error("‚ùå ERROR: La clave GEMINI_API_KEY no se encontr√≥ en st.secrets.")
        st.stop()
        
    client = genai.Client(api_key=api_key)
    
    # 2. Inicializar Embeddings para RAG
    embedding_function = GoogleGenerativeAIEmbeddings(
        model=EMBEDDING_MODEL_NAME, 
        google_api_key=api_key
    )
    return client, embedding_function

# Inicializar recursos globalmente (se ejecutan una vez)
client, embedding_function = initialize_gemini()


# FIX 3: La funci√≥n de carga ya no recibe argumentos para evitar el UnhashableParamError
@st.cache_resource 
def load_rag_database(): 
    """Carga la base de datos vectorial ChromaDB desde el directorio."""
    
    # Accedemos a la variable global 'embedding_function' ya inicializada
    global embedding_function 
    
    try:
        # Verifica si el directorio existe y si tiene archivos
        if os.path.exists(DB_PATH) and os.listdir(DB_PATH):
            db = Chroma(persist_directory=DB_PATH, 
                        embedding_function=embedding_function)
            st.success("‚úÖ Base de datos RAG cargada con √©xito.")
            return db
        else:
            st.warning("‚ö†Ô∏è Base de conocimiento RAG no encontrada. El tutor solo usar√° conocimiento general.")
            return None
    except Exception as e:
        st.error(f"‚ùå Error al cargar la BD: {e}. ¬øEst√° la carpeta `{DB_PATH}` en el repositorio?")
        return None

# Carga la base de datos
vector_db = load_rag_database()

# Iniciar la sesi√≥n de chat de Gemini (Persiste entre interacciones)
if 'chat_session' not in st.session_state:
    st.session_state.chat_session = client.chats.create(
        model=MODELO,
        config=chat_config
    )

# --- L√ìGICA DE RESPUESTA (CON RAG) ---

def generate_response(prompt):
    """Genera la respuesta del tutor, aumentada con RAG."""
    
    # 1. Recuperaci√≥n RAG
    contexto_rag = ""
    if vector_db:
        try:
            # Busca los 3 fragmentos m√°s relevantes en la base de datos
            docs = vector_db.similarity_search(prompt, k=3)
            contexto_rag = "\n\n".join([doc.page_content for doc in docs])
        except Exception as e:
            st.warning(f"Error en la consulta RAG: {e}")
    
    # 2. Inyecci√≥n de Contexto y Pregunta
    # El SYSTEM_INSTRUCTION (Reglas de MODO) se aplica a este prompt
    prompt_con_contexto = f"""
    [CONTEXTO DE TU BASE DE DATOS RAG]: {contexto_rag}
    [PREGUNTA DEL ALUMNO]: {prompt}
    """
    
    # 3. Respuesta de Gemini
    try:
        response = st.session_state.chat_session.send_message(prompt_con_contexto)
        return response.text
    except Exception as e:
        return f"‚ö†Ô∏è Error ({MODELO}): No se pudo generar la respuesta. Detalle: {str(e)}"

# --- INTERFAZ STREAMLIT ---

st.set_page_config(page_title="Genio Tutor RAG", page_icon="ü¶â", layout="wide")
st.title("ü¶â Genio: Tu Super Tutor IA RAG")
st.markdown(f"Modelo: `{MODELO}` | Base de Datos: {'‚úÖ Activa' if vector_db else '‚ùå Inactiva (Solo Conocimiento General)'}")

# Inicializar el historial de chat de Streamlit
if "messages" not in st.session_state:
    st.session_state.messages = []
    initial_message = "¬°Hola! Soy Genio. Estoy listo para ayudarte a aprender con el m√©todo socr√°tico. Si quieres una respuesta directa, escribe 'Modo: Resolver'. üß†"
    st.session_state.messages.append({"role": "assistant", "content": initial_message})

# Mostrar historial de mensajes
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Entrada de chat
if prompt := st.chat_input("Escribe aqu√≠ tu pregunta o tarea..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # Generar respuesta
    with st.spinner('Genio est√° pensando...'):
        response_text = generate_response(prompt)
    
    with st.chat_message("assistant"):
        st.markdown(response_text)
    
    st.session_state.messages.append({"role": "assistant", "content": response_text})

# app.py
# --- FIX PARA SQLITE EN STREAMLIT CLOUD ---
# Esto evita errores de versi√≥n de base de datos en la nube
__import__('pysqlite3')
import sys
sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')

# --- IMPORTS ---
import os
import streamlit as st
import fitz  # PyMuPDF para renderizar PDFs como im√°genes
import io
from PIL import Image
from google import genai
from google.genai import types

# Importaciones corregidas para las nuevas versiones de librer√≠as
from langchain_community.vectorstores import Chroma
from langchain_google_genai import GoogleGenerativeAIEmbeddings 

# --- CONFIGURACI√ìN DE VARIABLES GLOBALES ---
MODELO = "gemini-2.5-flash" 
DB_PATH = "genio_db_knowledge"  # Carpeta de la base de datos
PDF_FOLDER = "pdfs"             # Carpeta con los archivos PDF originales
EMBEDDING_MODEL_NAME = "text-embedding-004"

# --- PERSONALIDAD DEL TUTOR (MODO DUAL) ---
SYSTEM_INSTRUCTION = """
Eres "Genio", un tutor socr√°tico y asistente resolutivo. Tu objetivo es alternar entre dos modos, seg√∫n lo solicite el usuario.

--- REGLAS DE MODO ---
1. MODO ENSE√ëAR (Predeterminado):
   - Objetivo: Fomentar el aprendizaje y la autonom√≠a.
   - Regla de Oro: NUNCA dar la respuesta directa. Usar preguntas gu√≠a y el m√©todo socr√°tico.
   - Tono: Amable, paciente y motivador.

2. MODO RESOLVER (Gu√≠a Resolutivo):
   - Objetivo: Proveer la soluci√≥n clara o un resumen de hechos.
   - Activaci√≥n: Si el usuario dice 'Modo: Resolver', 'Dame la respuesta', o 'Resu√©lvelo'.
   - Regla de Oro: Ofrecer la respuesta directa, clara y paso a paso.

--- REGLAS GLOBALES ---
- Usa **negritas** para destacar las palabras clave.
- Utiliza el CONTEXTO RAG provisto para responder o guiar.
"""

# Configuraci√≥n del chat de Gemini
chat_config = types.GenerateContentConfig(
    system_instruction=SYSTEM_INSTRUCTION,
    temperature=0.7,
    max_output_tokens=1000
)

# --- INICIALIZACI√ìN DE RECURSOS (CACH√â) ---

@st.cache_resource
def initialize_gemini():
    """Inicializa el cliente de Gemini y la funci√≥n de embeddings."""
    try:
        api_key = st.secrets["GEMINI_API_KEY"]
    except KeyError:
        st.error("‚ùå ERROR: La clave GEMINI_API_KEY no se encontr√≥ en st.secrets.")
        st.stop()
        
    client = genai.Client(api_key=api_key)
    
    embedding_function = GoogleGenerativeAIEmbeddings(
        model=EMBEDDING_MODEL_NAME, 
        google_api_key=api_key
    )
    return client, embedding_function

# Inicializamos recursos globales para evitar problemas de cach√©
client, embedding_function = initialize_gemini()

@st.cache_resource 
def load_rag_database(): 
    """Carga la base de datos vectorial ChromaDB."""
    # Usamos la variable global para evitar 'UnhashableParamError'
    global embedding_function 
    
    try:
        if os.path.exists(DB_PATH) and os.listdir(DB_PATH):
            db = Chroma(persist_directory=DB_PATH, 
                        embedding_function=embedding_function)
            print("‚úÖ Base de datos RAG cargada correctamente.")
            return db
        else:
            print("‚ö†Ô∏è Base de datos no encontrada.")
            return None
    except Exception as e:
        print(f"‚ùå Error cargando DB: {e}")
        return None

vector_db = load_rag_database()

# --- FUNCI√ìN VISUAL: RENDERIZAR P√ÅGINA PDF ---
def render_pdf_page(filename, page_number):
    """Busca el PDF y convierte la p√°gina espec√≠fica en una imagen."""
    try:
        # Limpiamos el nombre del archivo
        clean_filename = os.path.basename(filename)
        pdf_path = os.path.join(PDF_FOLDER, clean_filename)
        
        if not os.path.exists(pdf_path):
            # Si no encuentra el archivo en la carpeta 'pdfs', no hace nada
            return None
        
        doc = fitz.open(pdf_path)
        # Validar que la p√°gina existe
        if 0 <= page_number < len(doc):
            page = doc.load_page(page_number)
            # Renderizar con Zoom x2 para mejor calidad
            pix = page.get_pixmap(matrix=fitz.Matrix(2, 2)) 
            img_data = pix.tobytes("png")
            return Image.open(io.BytesIO(img_data))
            
    except Exception as e:
        print(f"‚ö†Ô∏è Error renderizando PDF: {e}")
    return None

# --- L√ìGICA DE RESPUESTA (CON RAG Y VISUALES) ---

def generate_response_with_visuals(prompt):
    contexto_rag = ""
    sources_found = [] # Lista para guardar tuplas (archivo, p√°gina)
    
    # 1. B√∫squeda en la Base de Datos
    if vector_db:
        try:
            docs = vector_db.similarity_search(prompt, k=3)
            contexto_rag = "\n\n".join([doc.page_content for doc in docs])
            
            # Extraer fuentes √∫nicas para visualizaci√≥n
            for doc in docs:
                src = doc.metadata.get('source', '')
                page = doc.metadata.get('page', 0)
                if src:
                    sources_found.append((src, page))
                    
        except Exception as e:
            print(f"Error en b√∫squeda vectorial: {e}")
    
    # 2. Construcci√≥n del Prompt
    prompt_con_contexto = f"""
    [CONTEXTO RAG]: {contexto_rag}
    [PREGUNTA]: {prompt}
    """
    
    # 3. Generaci√≥n de Texto
    try:
        # Asegurar que la sesi√≥n de chat existe
        if 'chat_session' not in st.session_state:
            st.session_state.chat_session = client.chats.create(
                model=MODELO,
                config=chat_config
            )
            
        response_obj = st.session_state.chat_session.send_message(prompt_con_contexto)
        response_text = response_obj.text
    except Exception as e:
        response_text = f"‚ö†Ô∏è Error generando respuesta: {str(e)}"
        
    return response_text, sources_found

# --- INTERFAZ DE USUARIO STREAMLIT ---

st.set_page_config(page_title="Genio Visual", page_icon="ü¶â", layout="wide")

st.title("ü¶â Genio: Tu Super Tutor Visual")
st.markdown(f"**Estado RAG:** {'‚úÖ Activo' if vector_db else '‚ö†Ô∏è Inactivo (Solo conocimiento general)'}")

# Inicializar historial de chat
if "messages" not in st.session_state:
    st.session_state.messages = []
    st.session_state.messages.append({"role": "assistant", "content": "¬°Hola! Soy Genio. Preg√∫ntame lo que quieras y te mostrar√© de d√≥nde saco la informaci√≥n. üì∏"})

# 1. Mostrar mensajes antiguos
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])
        
        # Si el mensaje tiene im√°genes guardadas, las mostramos
        if "images" in message:
            for img_info in message["images"]:
                with st.expander(f"üîç Fuente: {img_info['name']} (P√°g {img_info['page'] + 1})"):
                    st.image(img_info['image'], use_column_width=True)

# 2. Input de Chat
if prompt := st.chat_input("Escribe tu pregunta aqu√≠..."):
    # Mostrar mensaje del usuario
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # Generar respuesta
    with st.spinner('Genio est√° pensando y buscando en los libros...'):
        response_text, sources = generate_response_with_visuals(prompt)
    
    # Procesar im√°genes nuevas (si las hay)
    images_to_save = []
    
    # Mostrar respuesta del asistente
    with st.chat_message("assistant"):
        st.markdown(response_text)
        
        if sources:
            # Filtrar duplicados para no mostrar la misma p√°gina 3 veces
            seen = set()
            unique_sources = [x for x in sources if not (x in seen or seen.add(x))]
            
            for src, page_num in unique_sources:
                img = render_pdf_page(src, page_num)
                if img:
                    clean_name = os.path.basename(src)
                    with st.expander(f"üì∏ Ver p√°gina original: {clean_name} (P√°g {page_num + 1})"):
                        st.image(img, caption=f"Fuente: {clean_name}", use_column_width=True)
                    
                    # Guardar imagen en memoria para el historial
                    images_to_save.append({'name': clean_name, 'page': page_num, 'image': img})
    
    # Guardar en el historial de sesi√≥n
    msg_data = {"role": "assistant", "content": response_text}
    if images_to_save:
        msg_data["images"] = images_to_save
    st.session_state.messages.append(msg_data)
